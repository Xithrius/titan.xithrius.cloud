x-default: &default
  restart: unless-stopped
  logging:
    driver: "json-file"
    options:
      max-size: "50m"

services:
  ollama:
    <<: *default

    image: ollama/ollama

    container_name: ollama

    volumes:
      - ollama:/root/.ollama

    ports:
      - "127.0.0.1:11434:11434"

    networks:
      - llm

    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]

  open-webui:
    <<: *default

    image: ghcr.io/open-webui/open-webui:main

    container_name: open-webui

    environment:
      OLLAMA_BASE_URL: http://ollama:11434

    volumes:
      - open-webui:/app/backend/data

    ports:
      - "0.0.0.0:3000:8080"

    networks:
      - llm
